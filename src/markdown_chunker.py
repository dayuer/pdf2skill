"""
Markdown AST Chunker â€” Phase 1B

åŸºäº Markdown æ ‡é¢˜å±‚çº§çš„è¯­ä¹‰åˆ‡åˆ†æ¨¡å—ã€‚
æ ¸å¿ƒåŸåˆ™ï¼šæŒ‰æ ‡é¢˜å±‚çº§åˆ‡åˆ†ï¼Œè€Œéå­—ç¬¦é•¿åº¦ã€‚ç¡®ä¿æ¯ä¸ª chunk è¯­ä¹‰å®Œæ•´ã€‚

åŠŸèƒ½ï¼š
1. AST è§£æï¼šä»¥ ##/### æ ‡é¢˜ä¸ºè¾¹ç•Œåˆ‡åˆ†é€»è¾‘å—
2. çˆ¶çº§ä¸Šä¸‹æ–‡æ³¨å…¥ï¼šä¸ºæ¯å—æ·»åŠ æ–‡æ¡£åç§° + ç« èŠ‚è·¯å¾„
3. å­—æ•°å¡ç‚¹ï¼šè¶…é•¿å—æŒ‰å¥å­è¾¹ç•ŒäºŒæ¬¡åˆ‡åˆ†ï¼ˆé˜ˆå€¼ 4000 å­—ï¼‰
4. è¿‡çŸ­åˆå¹¶ï¼šçŸ­å—å‘ä¸Šåˆå¹¶ï¼ˆé˜ˆå€¼ 200 å­—ï¼‰
5. ä¸‰çº§é™çº§ç­–ç•¥ï¼šæ ‡é¢˜åˆ‡åˆ† â†’ æ®µè½åˆ‡åˆ† â†’ å¥å­è¾¹ç•Œæ»‘åŠ¨çª—å£
6. è¯­ä¹‰å®Œæ•´æ€§ä¿è¯ï¼šæ‰€æœ‰åˆ‡åˆ†éƒ½åœ¨å¥å­/æ®µè½è¾¹ç•Œè¿›è¡Œï¼Œç»ä¸æˆªæ–­å¥å­
"""

from __future__ import annotations

import re
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional


# â”€â”€â”€â”€ é…ç½®å¸¸é‡ â”€â”€â”€â”€

# å•å—æœ€å¤§å­—æ•°ï¼šè¶…è¿‡åˆ™è§¦å‘äºŒæ¬¡åˆ‡åˆ†
MAX_CHUNK_CHARS = 4000

# å•å—æœ€å°å­—æ•°ï¼šä½äºåˆ™å‘ä¸Šåˆå¹¶åˆ°å‰ä¸€ä¸ªå—
MIN_CHUNK_CHARS = 200

# æ»‘åŠ¨çª—å£å¤§å°ï¼ˆé™çº§ç­–ç•¥ C ä½¿ç”¨ï¼‰
SLIDING_WINDOW_CHARS = 3000

# æ»‘åŠ¨çª—å£é‡å ç‡
SLIDING_OVERLAP_RATIO = 0.20

# AST åˆ‡åˆ†çš„æœ€å°æ ‡é¢˜æ•°é‡é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼è§¦å‘é™çº§ï¼‰
MIN_HEADING_COUNT = 5


@dataclass
class TextChunk:
    """ä¸€ä¸ªåˆ‡åˆ†åçš„æ–‡æœ¬å—"""

    content: str
    # çˆ¶çº§ä¸Šä¸‹æ–‡ï¼šæ–‡æ¡£åç§° + ç« èŠ‚è·¯å¾„
    context: str
    # åœ¨åŸæ–‡ä¸­çš„ä½ç½®ç´¢å¼•ï¼ˆä» 0 å¼€å§‹ï¼‰
    index: int
    # è¯¥å—çš„æ ‡é¢˜å±‚çº§è·¯å¾„ï¼Œå¦‚ ["ç¬¬ä¸‰ç«  æ•°æ®åº“è¿ç§»", "3.2 é¢„æ£€æ¸…å•"]
    heading_path: list[str] = field(default_factory=list)
    # å­—ç¬¦æ•°
    char_count: int = 0

    def __post_init__(self) -> None:
        self.char_count = len(self.content)


@dataclass
class ChunkResult:
    """åˆ‡åˆ†ç»“æœ"""

    chunks: list[TextChunk]
    # ä½¿ç”¨çš„åˆ‡åˆ†ç­–ç•¥
    strategy: str
    # åŸæ–‡æ€»å­—æ•°
    total_chars: int
    # æ–‡æ¡£åç§°
    doc_name: str


# â”€â”€â”€â”€ æ ‡é¢˜è§£æ â”€â”€â”€â”€

# åŒ¹é… Markdown æ ‡é¢˜è¡Œï¼š# ~ ######
_HEADING_RE = re.compile(r"^(#{1,6})\s+(.+)$", re.MULTILINE)


@dataclass
class _HeadingNode:
    """AST ä¸­çš„ä¸€ä¸ªæ ‡é¢˜èŠ‚ç‚¹"""

    level: int
    title: str
    # è¯¥æ ‡é¢˜åœ¨åŸæ–‡ä¸­çš„å­—ç¬¦åç§»é‡
    start_pos: int
    # è¯¥æ ‡é¢˜ç®¡è¾–çš„å†…å®¹ç»“æŸä½ç½®
    end_pos: int = 0


def _extract_headings(text: str) -> list[_HeadingNode]:
    """æå– Markdown æ–‡æœ¬ä¸­çš„æ‰€æœ‰æ ‡é¢˜èŠ‚ç‚¹"""
    headings: list[_HeadingNode] = []
    for m in _HEADING_RE.finditer(text):
        headings.append(
            _HeadingNode(
                level=len(m.group(1)),
                title=m.group(2).strip(),
                start_pos=m.start(),
            )
        )
    # è®¡ç®—æ¯ä¸ªæ ‡é¢˜ç®¡è¾–çš„å†…å®¹ç»“æŸä½ç½®ï¼ˆåˆ°ä¸‹ä¸€ä¸ªåŒçº§æˆ–æ›´é«˜çº§æ ‡é¢˜ä¹‹å‰ï¼‰
    for i, h in enumerate(headings):
        if i + 1 < len(headings):
            h.end_pos = headings[i + 1].start_pos
        else:
            h.end_pos = len(text)
    return headings


# â”€â”€â”€â”€ ä¸Šä¸‹æ–‡æ„å»º â”€â”€â”€â”€


def _build_context(doc_name: str, heading_path: list[str]) -> str:
    """æ„å»ºçˆ¶çº§ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
    parts = [f"æ–‡æ¡£ï¼šã€Š{doc_name}ã€‹"]
    if heading_path:
        parts.append(f"ç« èŠ‚ï¼š{' > '.join(heading_path)}")
    return " | ".join(parts)


def _build_heading_path(
    headings: list[_HeadingNode], current_idx: int
) -> list[str]:
    """ä¸ºå½“å‰æ ‡é¢˜èŠ‚ç‚¹æ„å»ºå®Œæ•´çš„å±‚çº§è·¯å¾„ï¼ˆå«æ‰€æœ‰ç¥–å…ˆæ ‡é¢˜ï¼‰"""
    if not headings or current_idx < 0:
        return []

    current = headings[current_idx]
    path = [current.title]

    # å‘å‰å›æº¯ï¼Œæ‰¾åˆ°æ¯ä¸€ä¸ªæ›´é«˜å±‚çº§çš„æ ‡é¢˜
    for i in range(current_idx - 1, -1, -1):
        if headings[i].level < current.level:
            path.insert(0, headings[i].title)
            current = headings[i]

    return path


# â”€â”€â”€â”€ ç­–ç•¥ Aï¼šæ ‡é¢˜çº§ AST åˆ‡åˆ† â”€â”€â”€â”€


def _chunk_by_headings(
    text: str,
    doc_name: str,
    headings: list[_HeadingNode],
    split_level: int = 2,
) -> list[TextChunk]:
    """
    æŒ‰æŒ‡å®šæ ‡é¢˜å±‚çº§åˆ‡åˆ†æ–‡æœ¬ã€‚

    Args:
        text: å®Œæ•´ Markdown æ–‡æœ¬
        doc_name: æ–‡æ¡£åç§°
        headings: é¢„æå–çš„æ ‡é¢˜èŠ‚ç‚¹åˆ—è¡¨
        split_level: ä»¥å“ªä¸ªæ ‡é¢˜å±‚çº§ä¸ºåˆ‡åˆ†è¾¹ç•Œï¼ˆé»˜è®¤ 2 = ##ï¼‰
    """
    # ç­›é€‰å‡ºä½œä¸ºåˆ‡åˆ†è¾¹ç•Œçš„æ ‡é¢˜ï¼ˆlevel <= split_levelï¼‰
    boundary_headings = [h for h in headings if h.level <= split_level]

    if not boundary_headings:
        # å¦‚æœæŒ‡å®šå±‚çº§æ— æ ‡é¢˜ï¼Œå°è¯•ç”¨æ‰€æœ‰æ ‡é¢˜
        boundary_headings = headings

    if not boundary_headings:
        return []

    raw_chunks: list[TextChunk] = []

    # å¤„ç†ç¬¬ä¸€ä¸ªæ ‡é¢˜ä¹‹å‰çš„å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
    preamble = text[: boundary_headings[0].start_pos].strip()
    if preamble and len(preamble) >= MIN_CHUNK_CHARS:
        raw_chunks.append(
            TextChunk(
                content=preamble,
                context=_build_context(doc_name, []),
                index=0,
                heading_path=[],
            )
        )

    # æŒ‰è¾¹ç•Œæ ‡é¢˜åˆ‡åˆ†
    for i, bh in enumerate(boundary_headings):
        # è¯¥æ ‡é¢˜ç®¡è¾–èŒƒå›´ï¼šä»æœ¬æ ‡é¢˜åˆ°ä¸‹ä¸€ä¸ªè¾¹ç•Œæ ‡é¢˜çš„èµ·å§‹ä½ç½®
        end = (
            boundary_headings[i + 1].start_pos
            if i + 1 < len(boundary_headings)
            else len(text)
        )
        chunk_text = text[bh.start_pos : end].strip()

        if not chunk_text:
            continue

        # åœ¨å®Œæ•´æ ‡é¢˜åˆ—è¡¨ä¸­æ‰¾åˆ°è¯¥æ ‡é¢˜çš„ç´¢å¼•ï¼Œç”¨äºæ„å»ºå±‚çº§è·¯å¾„
        full_idx = next(
            (j for j, h in enumerate(headings) if h.start_pos == bh.start_pos),
            -1,
        )
        heading_path = _build_heading_path(headings, full_idx)

        raw_chunks.append(
            TextChunk(
                content=chunk_text,
                context=_build_context(doc_name, heading_path),
                index=len(raw_chunks),
                heading_path=heading_path,
            )
        )

    return raw_chunks


# â”€â”€â”€â”€ ç­–ç•¥ Bï¼šæ®µè½çº§åˆ‡åˆ† â”€â”€â”€â”€

# æ®µè½åˆ†éš”ç¬¦ï¼šè¿ç»­ä¸¤ä¸ªä»¥ä¸Šæ¢è¡Œ
_PARAGRAPH_SEP_RE = re.compile(r"\n\s*\n")


def _chunk_by_paragraphs(
    text: str,
    doc_name: str,
    max_chars: int = MAX_CHUNK_CHARS,
) -> list[TextChunk]:
    """
    æŒ‰æ®µè½è¾¹ç•Œåˆ‡åˆ†ï¼Œåˆå¹¶çŸ­æ®µè½ç›´åˆ°è¾¾åˆ°å­—æ•°ä¸Šé™ã€‚
    é™çº§ç­–ç•¥ Bï¼šå½“æ ‡é¢˜æ•°é‡ä¸è¶³æ—¶ä½¿ç”¨ã€‚
    """
    paragraphs = [p.strip() for p in _PARAGRAPH_SEP_RE.split(text) if p.strip()]
    chunks: list[TextChunk] = []
    buffer: list[str] = []
    buffer_len = 0

    for para in paragraphs:
        # å¦‚æœåŠ å…¥å½“å‰æ®µè½åè¶…å‡ºä¸Šé™ï¼Œå…ˆåˆ·æ–° buffer
        if buffer and buffer_len + len(para) > max_chars:
            chunks.append(
                TextChunk(
                    content="\n\n".join(buffer),
                    context=_build_context(doc_name, []),
                    index=len(chunks),
                )
            )
            buffer = []
            buffer_len = 0

        buffer.append(para)
        buffer_len += len(para)

    # åˆ·æ–°å‰©ä½™ buffer
    if buffer:
        chunks.append(
            TextChunk(
                content="\n\n".join(buffer),
                context=_build_context(doc_name, []),
                index=len(chunks),
            )
        )

    return chunks


# â”€â”€â”€â”€ å¥å­è¾¹ç•Œåˆ‡åˆ†ï¼ˆä¿è¯è¯­ä¹‰å®Œæ•´æ€§ï¼‰ â”€â”€â”€â”€

# å¥å­ç»“æŸç¬¦ï¼šä¸­æ–‡å¥å·ã€é—®å·ã€æ„Ÿå¹å· + è‹±æ–‡å¥å· + æ¢è¡Œ
_SENTENCE_END_RE = re.compile(r'[ã€‚ï¼ï¼Ÿ\.!?]\s*|\n')


def _split_at_sentence_boundary(text: str, max_chars: int) -> list[str]:
    """
    åœ¨å¥å­è¾¹ç•Œå¤„åˆ‡åˆ†è¶…é•¿æ–‡æœ¬ï¼Œç¡®ä¿æ¯ä¸ªç‰‡æ®µè¯­ä¹‰å®Œæ•´ã€‚

    ç­–ç•¥ï¼šè´ªå¿ƒç§¯ç´¯å¥å­ï¼Œåœ¨ä¸è¶…è¿‡ max_chars çš„å‰æä¸‹å°½é‡å¤šè£…ã€‚
    æ¯æ¬¡åªåœ¨å¥å­ç»“æŸå¤„åˆ‡æ–­ï¼Œç»ä¸ä¼šåœ¨å¥å­ä¸­é—´æˆªæ–­ã€‚
    """
    if len(text) <= max_chars:
        return [text]

    parts: list[str] = []
    current_pos = 0

    while current_pos < len(text):
        # å¦‚æœå‰©ä½™æ–‡æœ¬ä¸è¶…è¿‡ max_charsï¼Œç›´æ¥å–å®Œ
        if current_pos + max_chars >= len(text):
            parts.append(text[current_pos:].strip())
            break

        # åœ¨ [current_pos, current_pos + max_chars] èŒƒå›´å†…æ‰¾æœ€åä¸€ä¸ªå¥å­ç»“æŸä½ç½®
        window = text[current_pos:current_pos + max_chars]
        last_sent_end = -1

        for m in _SENTENCE_END_RE.finditer(window):
            last_sent_end = m.end()

        if last_sent_end > 0 and last_sent_end > len(window) * 0.3:
            # åœ¨å¥å­è¾¹ç•Œå¤„åˆ‡æ–­ï¼ˆè‡³å°‘ä¿ç•™ 30% å†…å®¹é¿å…è¿‡çŸ­å—ï¼‰
            chunk_text = text[current_pos:current_pos + last_sent_end].strip()
            if chunk_text:
                parts.append(chunk_text)
            current_pos += last_sent_end
        else:
            # å®åœ¨æ‰¾ä¸åˆ°å¥å­è¾¹ç•Œï¼ˆæç«¯æƒ…å†µï¼‰ï¼Œåœ¨ç©ºæ ¼å¤„åˆ‡
            space_pos = window.rfind(' ')
            if space_pos > len(window) * 0.3:
                chunk_text = text[current_pos:current_pos + space_pos].strip()
                if chunk_text:
                    parts.append(chunk_text)
                current_pos += space_pos
            else:
                # æœ€ç»ˆé™çº§ï¼šç¡¬åˆ‡ï¼ˆç†è®ºä¸Šä¸ä¼šè§¦å‘ï¼‰
                parts.append(text[current_pos:current_pos + max_chars].strip())
                current_pos += max_chars

    return [p for p in parts if p]


# â”€â”€â”€â”€ ç­–ç•¥ Cï¼šæ»‘åŠ¨çª—å£åˆ‡åˆ† â”€â”€â”€â”€


def _chunk_by_sliding_window(
    text: str,
    doc_name: str,
    window_size: int = SLIDING_WINDOW_CHARS,
    overlap_ratio: float = SLIDING_OVERLAP_RATIO,
) -> list[TextChunk]:
    """
    å¥å­è¾¹ç•Œæ»‘åŠ¨çª—å£åˆ‡åˆ†ã€‚
    é™çº§ç­–ç•¥ Cï¼šçº¯æ–‡æœ¬å¢™ï¼Œæ— æ ‡é¢˜ä¹Ÿæ— æ¸…æ™°æ®µè½æ—¶ä½¿ç”¨ã€‚
    åœ¨å¥å­è¾¹ç•Œå¤„åˆ‡åˆ†ï¼Œä¿è¯è¯­ä¹‰å®Œæ•´æ€§ã€‚
    """
    # å…ˆæŒ‰å¥å­è¾¹ç•Œåˆ‡åˆ†ï¼Œå†ç»„è£…æˆçª—å£
    segments = _split_at_sentence_boundary(text, window_size)
    chunks: list[TextChunk] = []

    for seg in segments:
        if seg.strip():
            chunks.append(
                TextChunk(
                    content=seg.strip(),
                    context=_build_context(doc_name, []),
                    index=len(chunks),
                )
            )

    return chunks


# â”€â”€â”€â”€ åå¤„ç†ï¼šè¿‡çŸ­åˆå¹¶ + è¶…é•¿äºŒæ¬¡åˆ‡åˆ† â”€â”€â”€â”€


def _merge_short_chunks(chunks: list[TextChunk]) -> list[TextChunk]:
    """å°†è¿‡çŸ­çš„å—ï¼ˆ< MIN_CHUNK_CHARSï¼‰åˆå¹¶åˆ°å‰ä¸€ä¸ªå—"""
    if not chunks:
        return chunks

    merged: list[TextChunk] = [chunks[0]]

    for chunk in chunks[1:]:
        if chunk.char_count < MIN_CHUNK_CHARS and merged:
            # åˆå¹¶åˆ°å‰ä¸€ä¸ªå—
            prev = merged[-1]
            merged[-1] = TextChunk(
                content=prev.content + "\n\n" + chunk.content,
                context=prev.context,
                index=prev.index,
                heading_path=prev.heading_path,
            )
        else:
            merged.append(chunk)

    return merged


def _split_oversized_chunks(
    chunks: list[TextChunk], text: str, headings: list[_HeadingNode]
) -> list[TextChunk]:
    """å°†è¶…é•¿å—ï¼ˆ> MAX_CHUNK_CHARSï¼‰æŒ‰å­æ ‡é¢˜ã€æ®µè½æˆ–å¥å­è¾¹ç•Œè¿›è¡ŒäºŒæ¬¡åˆ‡åˆ†"""
    result: list[TextChunk] = []

    for chunk in chunks:
        if chunk.char_count <= MAX_CHUNK_CHARS:
            result.append(chunk)
            continue

        # å±‚çº§ 1ï¼šå°è¯•å—å†…å­æ ‡é¢˜åˆ‡åˆ†
        sub_headings = _extract_headings(chunk.content)
        if len(sub_headings) >= 2:
            sub_chunks = _chunk_by_headings(
                chunk.content, "", sub_headings, split_level=6,
            )
            for sc in sub_chunks:
                sc.context = chunk.context
                sc.heading_path = chunk.heading_path + sc.heading_path
                result.append(sc)
            continue

        # å±‚çº§ 2ï¼šæŒ‰æ®µè½åˆ‡åˆ†
        paras = [p.strip() for p in _PARAGRAPH_SEP_RE.split(chunk.content) if p.strip()]
        if len(paras) >= 2:
            sub_chunks = _chunk_by_paragraphs(
                chunk.content, "", max_chars=MAX_CHUNK_CHARS
            )
            for sc in sub_chunks:
                sc.context = chunk.context
                sc.heading_path = chunk.heading_path
                result.append(sc)
            continue

        # å±‚çº§ 3ï¼šæŒ‰å¥å­è¾¹ç•Œåˆ‡åˆ†ï¼ˆä¿è¯è¯­ä¹‰å®Œæ•´ï¼‰
        segments = _split_at_sentence_boundary(chunk.content, MAX_CHUNK_CHARS)
        for seg in segments:
            sc = TextChunk(
                content=seg,
                context=chunk.context,
                index=0,
                heading_path=chunk.heading_path,
            )
            result.append(sc)

    return result


# â”€â”€â”€â”€ å™ªéŸ³æ¸…æ´— â”€â”€â”€â”€

# éœ€è¦æ¸…é™¤çš„ç« èŠ‚å…³é”®è¯
_NOISE_SECTION_KEYWORDS = [
    "å‚è€ƒæ–‡çŒ®",
    "references",
    "bibliography",
    "è‡´è°¢",
    "acknowledgments",
    "acknowledgements",
]

# é¡µçœ‰é¡µè„š/çº¯é¡µç è¡Œ
_PAGE_NUMBER_RE = re.compile(r"^\s*-?\s*\d+\s*-?\s*$", re.MULTILINE)
_HEADER_FOOTER_RE = re.compile(
    r"^[-â”€â”â•]{3,}\s*$", re.MULTILINE
)


def clean_markdown(text: str) -> str:
    """
    Phase 1A æ¸…æ´—ï¼šç§»é™¤é¡µçœ‰é¡µè„šã€é¡µç ã€å‚è€ƒæ–‡çŒ®ã€è‡´è°¢ç­‰å™ªéŸ³å†…å®¹ã€‚

    Args:
        text: MinerU è¾“å‡ºçš„åŸå§‹ Markdown æ–‡æœ¬

    Returns:
        æ¸…æ´—åçš„ Markdown æ–‡æœ¬
    """
    # ç§»é™¤çº¯é¡µç è¡Œ
    text = _PAGE_NUMBER_RE.sub("", text)

    # ç§»é™¤åˆ†éš”çº¿ï¼ˆå¸¸è§é¡µçœ‰é¡µè„šæ ‡è®°ï¼‰
    text = _HEADER_FOOTER_RE.sub("", text)

    # ç§»é™¤å™ªéŸ³ç« èŠ‚ï¼ˆä»è¯¥æ ‡é¢˜åˆ°ä¸‹ä¸€ä¸ªåŒçº§æ ‡é¢˜æˆ–æ–‡æœ«ï¼‰
    headings = _extract_headings(text)
    sections_to_remove: list[tuple[int, int]] = []

    for i, h in enumerate(headings):
        title_lower = h.title.lower().strip()
        if any(kw in title_lower for kw in _NOISE_SECTION_KEYWORDS):
            sections_to_remove.append((h.start_pos, h.end_pos))

    # ä»åå¾€å‰åˆ é™¤ï¼Œé¿å…åç§»é‡é”™ä½
    for start, end in reversed(sections_to_remove):
        text = text[:start] + text[end:]

    # å‹ç¼©è¿ç»­ç©ºè¡Œä¸ºæœ€å¤šä¸¤ä¸ª
    text = re.sub(r"\n{3,}", "\n\n", text)

    return text.strip()


# â”€â”€â”€â”€ ä¸»å…¥å£ â”€â”€â”€â”€


# å¯¹è¯è½®æ¬¡è¾¹ç•Œæ£€æµ‹ï¼šTurn N / ğŸ‘¤ User / ğŸ¤– Assistant ç­‰æ¨¡å¼
_TURN_RE = re.compile(
    r"^(?:Turn\s+\d+|#{1,3}\s*(?:Turn|Round|è½®æ¬¡)\s*\d+|"
    r"(?:ğŸ‘¤|ğŸ§‘|ğŸ¤–|ğŸ’¬)\s*(?:User|Assistant|ç”¨æˆ·|åŠ©æ‰‹))",
    re.MULTILINE | re.IGNORECASE,
)

# ç†æƒ³å—å¤§å°ç›®æ ‡å€¼ï¼ˆç”¨äºè‡ªé€‚åº” split_level è¯„åˆ†ï¼‰
_TARGET_CHUNK_CHARS = 1500


def _auto_detect_split_level(
    text: str, headings: list[_HeadingNode]
) -> int:
    """
    è‡ªé€‚åº”æ£€æµ‹æœ€ä¼˜åˆ‡åˆ†å±‚çº§ã€‚
    éå† level 2â†’6ï¼Œé€‰æ‹©å¹³å‡å—å¤§å°æœ€æ¥è¿‘ TARGET çš„å±‚çº§ã€‚
    """
    text_len = len(text)
    best_level, best_score = 2, float("inf")

    for level in range(2, 7):
        boundaries = [h for h in headings if h.level <= level]
        if len(boundaries) < 2:
            continue
        avg_size = text_len / len(boundaries)
        score = abs(avg_size - _TARGET_CHUNK_CHARS)
        if score < best_score:
            best_score = score
            best_level = level

    return best_level


def _chunk_by_conversation_turns(
    text: str, doc_name: str
) -> list[TextChunk]:
    """æŒ‰å¯¹è¯è½®æ¬¡è¾¹ç•Œåˆ‡åˆ†ï¼ˆç”¨äºèŠå¤©è®°å½•/å¯¹è¯ä½“æ–‡æ¡£ï¼‰ã€‚"""
    positions = [m.start() for m in _TURN_RE.finditer(text)]
    if not positions:
        return []

    # ç¡®ä¿ä»æ–‡æœ¬èµ·å§‹ä½ç½®å¼€å§‹
    if positions[0] > 0:
        positions.insert(0, 0)

    chunks: list[TextChunk] = []
    for i, start in enumerate(positions):
        end = positions[i + 1] if i + 1 < len(positions) else len(text)
        content = text[start:end].strip()
        if not content or len(content) < MIN_CHUNK_CHARS:
            continue
        chunks.append(
            TextChunk(
                content=content,
                context=_build_context(doc_name, []),
                index=len(chunks),
            )
        )
    return chunks


def chunk_markdown(
    text: str,
    doc_name: str,
    *,
    split_level: int = 0,  # 0 = è‡ªåŠ¨æ£€æµ‹
    max_chars: int = MAX_CHUNK_CHARS,
    min_chars: int = MIN_CHUNK_CHARS,
    clean: bool = True,
) -> ChunkResult:
    """
    å°† Markdown æ–‡æœ¬åˆ‡åˆ†ä¸ºè¯­ä¹‰å®Œæ•´çš„æ–‡æœ¬å—ã€‚

    å››çº§é™çº§ç­–ç•¥ï¼š
    0. å¯¹è¯ä½“æ£€æµ‹ â†’ æŒ‰è½®æ¬¡åˆ‡åˆ†
    1. æ ‡é¢˜ â‰¥ 5 ä¸ª â†’ è‡ªé€‚åº” AST æ ‡é¢˜åˆ‡åˆ†
    2. æ ‡é¢˜ < 5 ä½†æ®µè½æ¸…æ™° â†’ æ®µè½è¾¹ç•Œåˆ‡åˆ†
    3. çº¯æ–‡æœ¬å¢™ â†’ æ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼ˆ20% é‡å ï¼‰

    Args:
        text: Markdown æ–‡æœ¬ï¼ˆé€šå¸¸ä¸º MinerU è¾“å‡ºï¼‰
        doc_name: æ–‡æ¡£åç§°ï¼ˆç”¨äºä¸Šä¸‹æ–‡æ³¨å…¥ï¼‰
        split_level: AST åˆ‡åˆ†çš„æ ‡é¢˜å±‚çº§ï¼ˆ0=è‡ªåŠ¨æ£€æµ‹ï¼‰
        max_chars: å•å—æœ€å¤§å­—æ•°
        min_chars: å•å—æœ€å°å­—æ•°ï¼ˆä½äºæ­¤å€¼å‘ä¸Šåˆå¹¶ï¼‰
        clean: æ˜¯å¦å…ˆæ‰§è¡Œå™ªéŸ³æ¸…æ´—

    Returns:
        ChunkResult åŒ…å«åˆ‡åˆ†åçš„ TextChunk åˆ—è¡¨å’Œå…ƒä¿¡æ¯
    """
    if clean:
        text = clean_markdown(text)

    total_chars = len(text)

    if not text.strip():
        return ChunkResult(
            chunks=[], strategy="empty", total_chars=0, doc_name=doc_name
        )

    # ç­–ç•¥ 0ï¼šå¯¹è¯ä½“æ£€æµ‹
    turn_matches = list(_TURN_RE.finditer(text))
    if len(turn_matches) >= 3:
        strategy = "conversation_turn"
        chunks = _chunk_by_conversation_turns(text, doc_name)
        if chunks:
            chunks = _split_oversized_chunks(chunks, text, [])
            for i, c in enumerate(chunks):
                c.index = i
            return ChunkResult(
                chunks=chunks, strategy=strategy,
                total_chars=total_chars, doc_name=doc_name,
            )

    # æå–æ ‡é¢˜
    headings = _extract_headings(text)

    # ä¸‰çº§é™çº§ç­–ç•¥é€‰æ‹©
    if len(headings) >= MIN_HEADING_COUNT:
        # ç­–ç•¥ Aï¼šè‡ªé€‚åº” AST æ ‡é¢˜åˆ‡åˆ†
        strategy = "heading_ast"
        level = split_level if split_level > 0 else _auto_detect_split_level(text, headings)
        chunks = _chunk_by_headings(text, doc_name, headings, level)
    elif _PARAGRAPH_SEP_RE.search(text):
        # ç­–ç•¥ Bï¼šæ®µè½è¾¹ç•Œåˆ‡åˆ†
        strategy = "paragraph"
        chunks = _chunk_by_paragraphs(text, doc_name, max_chars)
    else:
        # ç­–ç•¥ Cï¼šæ»‘åŠ¨çª—å£
        strategy = "sliding_window"
        chunks = _chunk_by_sliding_window(text, doc_name)

    # åå¤„ç†
    chunks = _merge_short_chunks(chunks)
    chunks = _split_oversized_chunks(chunks, text, headings)

    # é‡æ–°ç¼–å·ç´¢å¼•
    for i, c in enumerate(chunks):
        c.index = i

    return ChunkResult(
        chunks=chunks,
        strategy=strategy,
        total_chars=total_chars,
        doc_name=doc_name,
    )


# â”€â”€â”€â”€ CLI å·¥å…· â”€â”€â”€â”€


def chunk_file(filepath: str | Path, doc_name: Optional[str] = None) -> ChunkResult:
    """
    ä»æ–‡ä»¶è¯»å– Markdown å¹¶åˆ‡åˆ†ã€‚

    Args:
        filepath: Markdown æ–‡ä»¶è·¯å¾„
        doc_name: æ–‡æ¡£åç§°ï¼ˆé»˜è®¤ä½¿ç”¨æ–‡ä»¶åï¼‰
    """
    path = Path(filepath)
    if not path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{path}")

    text = path.read_text(encoding="utf-8")

    if doc_name is None:
        doc_name = path.stem

    return chunk_markdown(text, doc_name)


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("ç”¨æ³•ï¼špython markdown_chunker.py <markdown_file>")
        sys.exit(1)

    result = chunk_file(sys.argv[1])
    print(f"ç­–ç•¥ï¼š{result.strategy}")
    print(f"åŸæ–‡å­—æ•°ï¼š{result.total_chars}")
    print(f"åˆ‡åˆ†å—æ•°ï¼š{len(result.chunks)}")
    print("---")
    for chunk in result.chunks:
        print(f"[{chunk.index}] ({chunk.char_count}å­—) {chunk.context}")
        # é¢„è§ˆå‰ 80 å­—
        preview = chunk.content[:80].replace("\n", " ")
        print(f"    {preview}...")
        print()
