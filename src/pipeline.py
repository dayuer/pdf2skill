"""
Pipeline ä¸»æµç¨‹ç¼–æ’å™¨ â€” ä¸²è” Phase 0 â†’ Phase 4

æ‰§è¡Œæµï¼š
  æ–‡æ¡£åŠ è½½ â†’ æ¸…æ´—åˆ‡åˆ† â†’ è¯­ä¹‰ç²—ç­› â†’ Schema ç”Ÿæˆ â†’ å¹¶è¡Œæå– â†’ æ ¡éªŒ â†’ å»é‡åˆå¹¶ â†’ æ‰“åŒ…è¾“å‡º
"""

from __future__ import annotations

import asyncio
import json
import time
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Optional

from .config import PipelineConfig, config
from .document_loader import LoadResult, load_document
from .llm_client import AsyncDeepSeekClient, DeepSeekClient
from .markdown_chunker import ChunkResult, chunk_markdown
from .semantic_filter import filter_chunks
from .schema_generator import SkillSchema, generate_schema
from .sku_classifier import classify_batch
from .skill_extractor import extract_skills_batch
from .skill_generator import generate_claude_skills
from .glossary_extractor import save_glossary
from .skill_packager import package_skills
from .skill_reducer import cluster_skills, reduce_all_clusters
from .skill_validator import SkillValidator, SKUType, ValidatedSkill


@dataclass
class PipelineResult:
    """Pipeline æ‰§è¡Œç»“æœ"""

    # è¾“å‡ºç›®å½•è·¯å¾„
    output_dir: str
    # ç»Ÿè®¡ä¿¡æ¯
    total_chunks: int = 0
    filtered_chunks: int = 0
    raw_skills_count: int = 0
    valid_skills_count: int = 0
    failed_skills_count: int = 0
    clusters_count: int = 0
    final_skills_count: int = 0
    # Claude Skills è¾“å‡ºè·¯å¾„
    claude_skills_dir: str = ""
    # æ—¶é—´ç»Ÿè®¡ï¼ˆç§’ï¼‰
    elapsed_seconds: float = 0
    # é˜¶æ®µè€—æ—¶
    phase_timings: dict[str, float] = field(default_factory=dict)
    # æ–‡æ¡£ä¿¡æ¯
    doc_name: str = ""
    doc_format: str = ""
    chunk_strategy: str = ""
    # Schema ä¿¡æ¯
    book_type: str = ""
    domains: list[str] = field(default_factory=list)
    # SKU åˆ†ç±»ç»Ÿè®¡
    sku_stats: dict[str, int] = field(default_factory=dict)

    def summary(self) -> str:
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        lines = [
            f"ğŸ“„ æ–‡æ¡£ï¼š{self.doc_name}ï¼ˆ{self.doc_format}ï¼‰",
            f"ğŸ“Š åˆ‡åˆ†ç­–ç•¥ï¼š{self.chunk_strategy}ï¼Œå…± {self.total_chunks} å—ï¼ˆç­›å {self.filtered_chunks} å—ï¼‰",
            f"ğŸ” æå–ï¼š{self.raw_skills_count} ä¸ª Raw Skill â†’ {self.valid_skills_count} ä¸ªé€šè¿‡æ ¡éªŒ",
            f"âŒ æ ¡éªŒå¤±è´¥ï¼š{self.failed_skills_count} ä¸ª",
            f"ğŸ”— å»é‡èšç±»ï¼š{self.clusters_count} ç°‡ â†’ {self.final_skills_count} ä¸ª Final Skill",
            f"ğŸ“Š SKU åˆ†å¸ƒï¼š{self.sku_stats}",
            f"ğŸ“¦ è¾“å‡ºç›®å½•ï¼š{self.output_dir}",
            f"â±ï¸ æ€»è€—æ—¶ï¼š{self.elapsed_seconds:.1f}s",
        ]
        if self.phase_timings:
            lines.append("  é˜¶æ®µè€—æ—¶ï¼š")
            for phase, t in self.phase_timings.items():
                lines.append(f"    {phase}: {t:.1f}s")
        return "\n".join(lines)


def run_pipeline(
    filepath: str | Path,
    *,
    book_name: Optional[str] = None,
    schema_override: Optional[str | Path] = None,
    output_dir: Optional[str | Path] = None,
    max_chunks: Optional[int] = None,
    cfg: Optional[PipelineConfig] = None,
) -> PipelineResult:
    """
    åŒæ­¥æ‰§è¡Œå®Œæ•´ Pipelineã€‚

    Args:
        filepath: æ–‡æ¡£è·¯å¾„ï¼ˆPDF/TXT/EPUBï¼‰
        book_name: ä¹¦åï¼ˆé»˜è®¤ä½¿ç”¨æ–‡ä»¶åï¼‰
        schema_override: å¯é€‰çš„ Schema JSON æ–‡ä»¶è·¯å¾„ï¼ˆäººå·¥ Overrideï¼‰
        output_dir: è¾“å‡ºç›®å½•
        max_chunks: æœ€å¤§å¤„ç†å—æ•°ï¼ˆè¶…å¤§æ–‡æ¡£é‡‡æ ·æ¨¡å¼ï¼‰ï¼ŒNone è¡¨ç¤ºå…¨é‡
        cfg: Pipeline é…ç½®ï¼ˆé»˜è®¤ä½¿ç”¨å…¨å±€é…ç½®ï¼‰

    Returns:
        PipelineResult æ‰§è¡Œç»“æœ
    """
    return asyncio.run(
        run_pipeline_async(
            filepath,
            book_name=book_name,
            schema_override=schema_override,
            output_dir=output_dir,
            max_chunks=max_chunks,
            cfg=cfg,
        )
    )


async def run_pipeline_async(
    filepath: str | Path,
    *,
    book_name: Optional[str] = None,
    schema_override: Optional[str | Path] = None,
    output_dir: Optional[str | Path] = None,
    max_chunks: Optional[int] = None,
    cfg: Optional[PipelineConfig] = None,
) -> PipelineResult:
    """
    å¼‚æ­¥æ‰§è¡Œå®Œæ•´ Pipelineã€‚

    æ•°æ®æµï¼š
    1. Phase 0: æ–‡æ¡£åŠ è½½ + Schema ç”Ÿæˆ
    2. Phase 1: æ¸…æ´— + AST åˆ‡åˆ†
    3. Phase 2: å¹¶è¡Œ Skill æå– + æ ¡éªŒ
    4. Phase 3: å‘é‡å»é‡ + R1 åˆå¹¶
    5. Phase 4: æ‰“åŒ…è¾“å‡º
    """
    if cfg is None:
        cfg = config

    t_start = time.monotonic()
    timings: dict[str, float] = {}
    result = PipelineResult(output_dir="")

    # â”€â”€ Phase 0Aï¼šæ–‡æ¡£åŠ è½½ â”€â”€
    t0 = time.monotonic()
    print(f"ğŸ“„ åŠ è½½æ–‡æ¡£ï¼š{filepath}")
    load_result = load_document(filepath)
    doc_name = book_name or load_result.doc_name
    result.doc_name = doc_name
    result.doc_format = load_result.format.value

    if load_result.warnings:
        for w in load_result.warnings:
            print(f"  âš ï¸ {w}")

    timings["æ–‡æ¡£åŠ è½½"] = time.monotonic() - t0

    # â”€â”€ Phase 1ï¼šæ¸…æ´— + AST åˆ‡åˆ† â”€â”€
    t0 = time.monotonic()
    print(f"âœ‚ï¸ åˆ‡åˆ†æ–‡æœ¬...")
    chunk_result = chunk_markdown(
        load_result.markdown,
        doc_name,
        split_level=cfg.chunk.split_level,
        max_chars=cfg.chunk.max_chunk_chars,
        min_chars=cfg.chunk.min_chunk_chars,
    )
    result.total_chunks = len(chunk_result.chunks)
    result.chunk_strategy = chunk_result.strategy
    print(f"  ç­–ç•¥ï¼š{chunk_result.strategy}ï¼Œ{result.total_chunks} ä¸ªæ–‡æœ¬å—")
    timings["åˆ‡åˆ†æ¸…æ´—"] = time.monotonic() - t0

    if not chunk_result.chunks:
        print("âŒ åˆ‡åˆ†ç»“æœä¸ºç©ºï¼Œæ— æ³•ç»§ç»­")
        result.elapsed_seconds = time.monotonic() - t_start
        result.phase_timings = timings
        return result

    # â”€â”€ Phase 1Cï¼šè¯­ä¹‰å¯†åº¦ç²—ç­› â”€â”€
    t0 = time.monotonic()
    print(f"ğŸ§¹ è¯­ä¹‰å¯†åº¦ç²—ç­›...")
    filter_result = filter_chunks(chunk_result.chunks)
    result.filtered_chunks = filter_result.kept_count
    print(f"  ä¿ç•™ï¼š{filter_result.kept_count}ï¼Œä¸¢å¼ƒï¼š{filter_result.dropped_count}")
    if filter_result.dropped:
        for d in filter_result.dropped[:3]:
            preview = d.chunk.content[:40].replace('\n', ' ')
            print(f"    ğŸ—‘ï¸ {d.reason}ï¼š{preview}...")
    chunks_to_process = filter_result.kept
    timings["è¯­ä¹‰ç²—ç­›"] = time.monotonic() - t0

    # â”€â”€ å¤§æ–‡æ¡£é‡‡æ · â”€â”€
    if max_chunks and len(chunks_to_process) > max_chunks:
        # å‡åŒ€é‡‡æ ·ï¼Œä¿æŒæ–‡æ¡£è¦†ç›–
        step = len(chunks_to_process) / max_chunks
        sampled = [chunks_to_process[int(i * step)] for i in range(max_chunks)]
        print(f"ğŸ“ å¤§æ–‡æ¡£é‡‡æ ·ï¼š{len(chunks_to_process)} â†’ {len(sampled)} å—ï¼ˆå‡åŒ€é‡‡æ ·ï¼‰")
        chunks_to_process = sampled

    # â”€â”€ Phase 0Bï¼šSchema ç”Ÿæˆ â”€â”€
    t0 = time.monotonic()
    if schema_override:
        print(f"ğŸ“‹ ä½¿ç”¨äººå·¥ Schemaï¼š{schema_override}")
        schema = SkillSchema.load(schema_override)
    else:
        print(f"ğŸ§  R1 æ¨æ–­ Schema...")
        sync_client = DeepSeekClient()
        schema = generate_schema(load_result.markdown, doc_name, client=sync_client)
        print(f"  ä¹¦ç±ç±»å‹ï¼š{schema.book_type}")
        print(f"  é¢†åŸŸï¼š{schema.domains}")

    result.book_type = schema.book_type
    result.domains = schema.domains
    timings["Schemaç”Ÿæˆ"] = time.monotonic() - t0

    # â”€â”€ Phase 2ï¼šå¹¶è¡Œ Skill æå– â”€â”€
    t0 = time.monotonic()
    print(f"â›ï¸ å¹¶è¡Œæå– Skillï¼ˆå¹¶å‘æ•°ï¼š{cfg.max_concurrent_requests}ï¼‰...")
    async_client = AsyncDeepSeekClient()
    raw_skills = await extract_skills_batch(
        chunks_to_process,
        schema,
        client=async_client,
    )
    result.raw_skills_count = len(raw_skills)
    print(f"  æå–åˆ° {len(raw_skills)} ä¸ª Raw Skill")
    timings["Skillæå–"] = time.monotonic() - t0

    if not raw_skills:
        print("âš ï¸ æœªæå–åˆ°ä»»ä½• Skill")
        result.elapsed_seconds = time.monotonic() - t_start
        result.phase_timings = timings
        return result

    # â”€â”€ Phase 2Bï¼šæ ¡éªŒ â”€â”€
    t0 = time.monotonic()
    print(f"ğŸ” æ ¡éªŒ Skill...")
    validator = SkillValidator()
    source_texts = [c.content for c in chunks_to_process]

    # ä¸ºæ¯ä¸ª raw_skill æ‰¾åˆ°å¯¹åº”çš„ source_text
    raw_source_texts = []
    for rs in raw_skills:
        if rs.source_chunk_index < len(source_texts):
            raw_source_texts.append(source_texts[rs.source_chunk_index])
        else:
            raw_source_texts.append(None)

    passed, failed = validator.validate_batch(raw_skills, source_texts=raw_source_texts)
    result.valid_skills_count = len(passed)
    result.failed_skills_count = len(failed)
    print(f"  âœ… é€šè¿‡ï¼š{len(passed)}ï¼ŒâŒ å¤±è´¥ï¼š{len(failed)}")

    if failed:
        for f in failed[:3]:
            print(f"    âŒ {f.name or '(unnamed)'}: {f.warnings}")

    timings["æ ¡éªŒ"] = time.monotonic() - t0

    if not passed:
        print("âš ï¸ æ—  Skill é€šè¿‡æ ¡éªŒ")
        result.elapsed_seconds = time.monotonic() - t_start
        result.phase_timings = timings
        return result

    # â”€â”€ Phase 3ï¼šå»é‡ + R1 åˆå¹¶ â”€â”€
    t0 = time.monotonic()
    print(f"ğŸ”— å‘é‡å»é‡èšç±»...")
    clusters = cluster_skills(passed, threshold=cfg.dedup_similarity_threshold)
    result.clusters_count = len(clusters)
    multi_clusters = [c for c in clusters if len(c.skills) > 1]
    print(f"  {len(clusters)} ç°‡ï¼ˆå…¶ä¸­ {len(multi_clusters)} ç°‡éœ€è¦åˆå¹¶ï¼‰")

    if multi_clusters:
        print(f"  ğŸ§  R1 åˆå¹¶åŒç±»é¡¹...")
        final_skills = await reduce_all_clusters(clusters, client=async_client)
    else:
        final_skills = [c.skills[0] for c in clusters]

    result.final_skills_count = len(final_skills)
    timings["å»é‡åˆå¹¶"] = time.monotonic() - t0

    # â”€â”€ Phase 3.5ï¼šSKU åˆ†ç±» â”€â”€
    t0 = time.monotonic()
    print(f"ğŸ·ï¸ SKU åˆ†ç±»...")
    final_skills = classify_batch(final_skills)
    sku_stats = {}
    for s in final_skills:
        sku_stats[s.sku_type.value] = sku_stats.get(s.sku_type.value, 0) + 1
    result.sku_stats = sku_stats
    print(f"  ğŸ“‹ factual: {sku_stats.get('factual', 0)} | âš™ï¸ procedural: {sku_stats.get('procedural', 0)} | ğŸ”— relational: {sku_stats.get('relational', 0)}")
    timings["SKUåˆ†ç±»"] = time.monotonic() - t0

    # â”€â”€ Phase 4ï¼šæ‰“åŒ…è¾“å‡º â”€â”€
    t0 = time.monotonic()
    print(f"ğŸ“¦ æ‰“åŒ…è¾“å‡º...")
    out_path = package_skills(
        final_skills,
        doc_name,
        output_dir=output_dir or cfg.output_dir,
    )
    result.output_dir = str(out_path)
    print(f"  è¾“å‡ºç›®å½•ï¼š{out_path}")
    timings["æ‰“åŒ…è¾“å‡º"] = time.monotonic() - t0

    # â”€â”€ Phase 5ï¼šClaude Skills ç”Ÿæˆ â”€â”€
    t0 = time.monotonic()
    print(f"ğŸ¯ ç”Ÿæˆ Claude Skills...")
    skills_path = generate_claude_skills(
        final_skills,
        doc_name,
        output_dir=output_dir or cfg.output_dir,
    )
    result.claude_skills_dir = str(skills_path)
    print(f"  Claude Skills ç›®å½•ï¼š{skills_path}")
    timings["Claude Skills"] = time.monotonic() - t0

    # â”€â”€ Phase 6ï¼šæœ¯è¯­è¡¨æå– â”€â”€
    t0 = time.monotonic()
    glossary_path = save_glossary(
        final_skills, doc_name, output_dir=output_dir or cfg.output_dir
    )
    if glossary_path and glossary_path.exists():
        print(f"ğŸ“š æœ¯è¯­è¡¨ï¼š{glossary_path}")
    timings["æœ¯è¯­è¡¨"] = time.monotonic() - t0

    # â”€â”€ å®Œæˆ â”€â”€
    result.elapsed_seconds = time.monotonic() - t_start
    result.phase_timings = timings
    print(f"\n{'=' * 50}")
    print(result.summary())

    return result


# â”€â”€â”€â”€ CLI å…¥å£ â”€â”€â”€â”€

if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("ç”¨æ³•ï¼špython -m src.pipeline <æ–‡æ¡£è·¯å¾„> [ä¹¦å] [--max-chunks N]")
        print("æ”¯æŒæ ¼å¼ï¼šPDF, TXT, EPUB")
        sys.exit(1)

    filepath = sys.argv[1]
    name = None
    mc = None

    # è§£æå‚æ•°
    args = sys.argv[2:]
    i = 0
    while i < len(args):
        if args[i] == "--max-chunks" and i + 1 < len(args):
            mc = int(args[i + 1])
            i += 2
        elif name is None:
            name = args[i]
            i += 1
        else:
            i += 1

    result = run_pipeline(filepath, book_name=name, max_chunks=mc)
    print(f"\nå®Œæˆã€‚æœ€ç»ˆ Skill æ•°é‡ï¼š{result.final_skills_count}")
